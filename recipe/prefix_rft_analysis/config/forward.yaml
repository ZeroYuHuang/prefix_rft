data:
  micro_batch_size_per_gpu: 4  # this is also val batch size
  train_files: ~/data/gsm8k/train.parquet
  # Single-turn settings
  prompt_key: question
  response_key: answer
  prompt_dict_keys: null
  response_dict_keys: null
  max_length: 1024
  truncation: error
  balance_dp_token: False
  chat_template: null
  use_shm: False
model:
  partial_pretrain: ~/models/gemma-1.1-7b-it
  use_shm: False
  fsdp_config:
    model_dtype: fp32
    wrap_policy:
      min_num_params: 0
    cpu_offload: False
    offload_params: False
  external_lib: null
  enable_gradient_checkpointing: True
  trust_remote_code: False
  use_liger: False
  strategy: fsdp2
trainer:
  default_local_dir: /mnt/jfs2/cth/085b13/_checkpoints/${trainer.project_name}/${trainer.experiment_name}
  default_hdfs_dir: null
  resume_path: null
  project_name: gsm8k-sft
  experiment_name: test
  total_training_steps: null
  logger: [ 'console', 'wandb' ]
  seed: 1
  nnodes: 1
  n_gpus_per_node: 8
output_file: forward.json